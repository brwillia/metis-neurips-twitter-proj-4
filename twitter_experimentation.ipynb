{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring NeurIPS (2019) tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T12:25:31.687138Z",
     "iopub.status.busy": "2020-11-13T12:25:31.686796Z",
     "iopub.status.idle": "2020-11-13T12:25:31.702743Z",
     "shell.execute_reply": "2020-11-13T12:25:31.701713Z",
     "shell.execute_reply.started": "2020-11-13T12:25:31.687109Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# snscrape --max-results 5000 twitter-hashtag neurips > neurips_tag_5000.txt\n",
    "# snscrape --max-results 5000 twitter-hashtag neurips2020 > neurips2020_tag_5000.txt\n",
    "# snscrape --max-results 5000 twitter-hashtag neurips2019 > neurips2019_tag_5000.txt\n",
    "# snscrape --max-results 5000 twitter-hashtag neurips2018 > neurips2018_tag_5000.txt\n",
    "# snscrape --max-results 5000 twitter-hashtag nips2018 > nips2018_tag_5000.txt\n",
    "# snscrape --max-results 5000 twitter-hashtag nips2017 > nips2017_tag_5000.txt\n",
    "\n",
    "# snscrape --max-results 5000 twitter-search nips > nips_search_5000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try nouns first\n",
    "tweet_list = open(\"neurips_tag_scrape_1000.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "username_dict = defaultdict(int)\n",
    "for line in tweet_list:\n",
    "    username = line.replace(\"https://twitter.com/\", \"\").split('/')[0]\n",
    "    username_dict[username]+=1\n",
    "\n",
    "user_df = pd.DataFrame([[k, v] for k, v in username_dict.items()], \n",
    "                       columns=['id', 'count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BorisAKnyazev</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flareaudio</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michellemng</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  count\n",
       "0  BorisAKnyazev      2\n",
       "1     flareaudio      9\n",
       "2    michellemng      1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>anoushnajarian</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>thinkmariya</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AminerScholar</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>AISaturdaysIB</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>AnimaAnandkumar</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>Tijazz94</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flareaudio</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>FmFrancoise</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>Abebab</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>AtticScientist</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  count\n",
       "7     anoushnajarian     44\n",
       "453      thinkmariya     29\n",
       "9      AminerScholar     27\n",
       "484    AISaturdaysIB     15\n",
       "46   AnimaAnandkumar     12\n",
       "489         Tijazz94     11\n",
       "1         flareaudio      9\n",
       "376      FmFrancoise      9\n",
       "332           Abebab      9\n",
       "331   AtticScientist      8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.sort_values(\"count\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor index, row in user_df.iterrows():\\n    !snscrape --max-results 200 twitter-user {row[\\'id\\']}\\n\\n! snscrape --max-results 5000 twitter-search \"#neurips2019 since:2015-01-01 until:2020-03-15\" > neurips2019_tweets.txt\\n    \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for index, row in user_df.iterrows():\n",
    "    !snscrape --max-results 200 twitter-user {row['id']}\n",
    "\n",
    "! snscrape --max-results 5000 twitter-search \"#neurips2019 since:2015-01-01 until:2020-03-15\" > neurips2019_tweets.txt\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>links</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/boredyannlecun/status/1238...</td>\n",
       "      <td>1238623609846935553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/ionispharma/status/1310596...</td>\n",
       "      <td>1310596334613868545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://twitter.com/AnalyticaGlobal/status/123...</td>\n",
       "      <td>1238497697746890756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://twitter.com/Oracle/status/123849521419...</td>\n",
       "      <td>1238495214190432261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://twitter.com/OracleBelux/status/1238433...</td>\n",
       "      <td>1238433480633171969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               links                   id\n",
       "0  https://twitter.com/boredyannlecun/status/1238...  1238623609846935553\n",
       "1  https://twitter.com/ionispharma/status/1310596...  1310596334613868545\n",
       "2  https://twitter.com/AnalyticaGlobal/status/123...  1238497697746890756\n",
       "3  https://twitter.com/Oracle/status/123849521419...  1238495214190432261\n",
       "4  https://twitter.com/OracleBelux/status/1238433...  1238433480633171969"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_url = pd.read_csv(\"neurips2019_tweets.txt\", index_col= None, header = None, names = [\"links\"])\n",
    "af = lambda x: x[\"links\"].split(\"/\")[-1]\n",
    "tweet_url['id'] = tweet_url.apply(af, axis=1)\n",
    "tweet_url.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = \"FRVGuMgEuhJNQxcMUgLT3MMao\" \n",
    "consumer_secret = \"rFWAW4A4yys6jvPcGdBY4K5wNfFA9Iy2BIQMqr9yhS2I9tYxn8\" \n",
    "access_token = \"939281442621743104-MgHDnMelzUZHeh2Sui0nkJ1oTYfcWHy\" \n",
    "access_token_secret = \"sjQkY0dye7IPJB4MmkSRqatN49gA7n7bNpgdR7liUeLOf\"\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tweet_url['id'].tolist()\n",
    "total_count = len(ids)\n",
    "chunks = (total_count - 1) // 50 + 1\n",
    "\n",
    "def fetch_tw(ids):\n",
    "    list_of_tw_status = api.statuses_lookup(ids, tweet_mode= \"extended\")\n",
    "    empty_data = pd.DataFrame()\n",
    "    for status in list_of_tw_status:\n",
    "            tweet_elem = {\"tweet_id\": int(status.id),\n",
    "                     \"screen_name\": status.user.screen_name,\n",
    "                     \"tweet\":status.full_text,\n",
    "                     \"date\":status.created_at}\n",
    "            empty_data = empty_data.append(tweet_elem, ignore_index = True)\n",
    "    empty_data.to_csv(\"neurips2019_tweets.csv\", mode=\"a\")\n",
    "\n",
    "# for i in range(chunks):\n",
    "#         batch = ids[i*50:(i+1)*50]\n",
    "#         result = fetch_tw(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting those tweets into Pandas for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>2020-02-20 21:46:33</td>\n",
       "      <td>JoinRobinly</td>\n",
       "      <td>@josh_tobin_ , former researcher @OpenAI &amp;amp;...</td>\n",
       "      <td>1.230609711864529e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2020-03-09 16:05:16</td>\n",
       "      <td>Oracle</td>\n",
       "      <td>What happens to accuracy when we try to increa...</td>\n",
       "      <td>1.2370468083392635e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>2020-02-21 08:18:33</td>\n",
       "      <td>ShayneLibby</td>\n",
       "      <td>Oracle: How can you improve both privacy and a...</td>\n",
       "      <td>1.2307687603880796e+18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date  screen_name  \\\n",
       "0.0  2020-02-20 21:46:33  JoinRobinly   \n",
       "1.0  2020-03-09 16:05:16       Oracle   \n",
       "2.0  2020-02-21 08:18:33  ShayneLibby   \n",
       "\n",
       "                                                 tweet                tweet_id  \n",
       "0.0  @josh_tobin_ , former researcher @OpenAI &amp;...   1.230609711864529e+18  \n",
       "1.0  What happens to accuracy when we try to increa...  1.2370468083392635e+18  \n",
       "2.0  Oracle: How can you improve both privacy and a...  1.2307687603880796e+18  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2019 = pd.read_csv(\"neurips2019_tweets.csv\", index_col=0)\n",
    "df_2019.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bgalbraith</th>\n",
       "      <td>483</td>\n",
       "      <td>483</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JFutoma</th>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen_name</th>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RahelJhirad</th>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeetVancouver</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>akbirkhan</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>akashmehra</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ajitndesai</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aifredhealth</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuNaoya</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1907 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  tweet  tweet_id\n",
       "screen_name                         \n",
       "bgalbraith      483    483       483\n",
       "JFutoma         369    369       369\n",
       "screen_name     299    299       299\n",
       "RahelJhirad     237    237       237\n",
       "MeetVancouver   201    201       201\n",
       "...             ...    ...       ...\n",
       "akbirkhan         3      3         3\n",
       "akashmehra        3      3         3\n",
       "ajitndesai        3      3         3\n",
       "aifredhealth      3      3         3\n",
       "zuNaoya           3      3         3\n",
       "\n",
       "[1907 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2019.groupby('screen_name').count().sort_values(by='tweet',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "def cleanTweet(tweet):\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    tweet = re.sub(r'\\&\\w*;', '', tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    # Remove tickers\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # To lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # Remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
    "    # Remove Punctuation and split 's, 't, 've with a space for filter\n",
    "    tweet = re.sub(r'[' + string.punctuation.replace('@', '') + ']+', ' ', tweet)\n",
    "    # Remove whitespace (including new line characters)\n",
    "    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
    "    # Remove single space remaining at the front of the tweet.\n",
    "    tweet = tweet.lstrip(' ') \n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "    tweet = ''.join(c for c in tweet if c <= '\\uFFFF')\n",
    "    return tweet\n",
    "\n",
    "# Need to stem and/or lemmatize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>2020-02-20 21:46:33</td>\n",
       "      <td>JoinRobinly</td>\n",
       "      <td>@josh_tobin_ , former researcher @OpenAI &amp;amp;...</td>\n",
       "      <td>1.230609711864529e+18</td>\n",
       "      <td>former researcher phd shared highlights from h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2020-03-09 16:05:16</td>\n",
       "      <td>Oracle</td>\n",
       "      <td>What happens to accuracy when we try to increa...</td>\n",
       "      <td>1.2370468083392635e+18</td>\n",
       "      <td>what happens to accuracy when we try to increa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>2020-02-21 08:18:33</td>\n",
       "      <td>ShayneLibby</td>\n",
       "      <td>Oracle: How can you improve both privacy and a...</td>\n",
       "      <td>1.2307687603880796e+18</td>\n",
       "      <td>oracle how can you improve both privacy and ac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date  screen_name  \\\n",
       "0.0  2020-02-20 21:46:33  JoinRobinly   \n",
       "1.0  2020-03-09 16:05:16       Oracle   \n",
       "2.0  2020-02-21 08:18:33  ShayneLibby   \n",
       "\n",
       "                                                 tweet  \\\n",
       "0.0  @josh_tobin_ , former researcher @OpenAI &amp;...   \n",
       "1.0  What happens to accuracy when we try to increa...   \n",
       "2.0  Oracle: How can you improve both privacy and a...   \n",
       "\n",
       "                   tweet_id                                        clean_tweet  \n",
       "0.0   1.230609711864529e+18  former researcher phd shared highlights from h...  \n",
       "1.0  1.2370468083392635e+18  what happens to accuracy when we try to increa...  \n",
       "2.0  1.2307687603880796e+18  oracle how can you improve both privacy and ac...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2019['clean_tweet'] = df_2019['tweet'].apply(cleanTweet)\n",
    "df_2019.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = df_2019.fillna(value='')\n",
    "corpus = df_2019['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within a few clicks a user goes from searching for an innocuous subject activated charcoal to believing activated charcoal is good for you huge change in belief system as a result of google search rankings neurips2019\n",
      "\n",
      "creative gans for generating poems lyrics and metaphors by and at neurips4creativity neurips2019 congrats to all presenters \n",
      "\n",
      "7 sat 4 15 pm west 121 122 on science meets engineering of deep learning panel with aparna lakshmiratan moderated but never in moderation by and finally have a great neurips2019  \n",
      "\n",
      "16 years old boy went to do ritual and the police arrest him to read more about this kindly click here missworld magicshopinosaka neurips2019 nffc nigeria pakistan phcn powerrangers qatar2019 quotes realsociedadbarca religiousdiscriminationbill\n",
      "\n",
      "heard at the end of a talk at neurips2019 “finally for those interested in neuroscience ”\n",
      "\n",
      "delighted to incoming to neurips2019 i’ll be giving talks at deep rl at 1 30pm west exhibition hall c and ml w guarantees at 4 45pm west ballrm b \n",
      "\n",
      "domain adaptation techniques can improve per user accuracy in a federatedlearning setup and buffers against accuracy loss from privacy noise daniel peterson presents at neurips2019 \n",
      "\n",
      "weight agnostic neural networks ai machinelearning artificialintelligence neurips2019 via \n",
      "\n",
      "and so it begins neurips2019 \n",
      "\n",
      "reminding us at the neurips2019 workshop on ai for social good that good is a contested notion and that testing for accuracy is already valuation practice \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in corpus.sample(10).values:\n",
    "    print(tweet)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA with TruncatedSVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15299, 11108)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS.union(['neurips', 'neurips2019'])\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "doc_word = vectorizer.fit_transform(corpus)\n",
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF prep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15299, 11108)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stop_words)\n",
    "tweet_word_matrix = tfidf.fit_transform(corpus)\n",
    "print(tweet_word_matrix.shape)\n",
    "vocab = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert `.toarray()` because the vectorizer returns a sparse matrix.\n",
    "# For a big corpus, we would skip the dataframe and keep the output sparse.\n",
    "dtm_lsa = pd.DataFrame(doc_word.toarray(), index=corpus, columns=vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01541733, 0.01494579])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Latent Semantic Analysis (LSA) is just another name for Singular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "lsa = TruncatedSVD(2)\n",
    "doc_topic = lsa.fit_transform(doc_word)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000mph</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>02476</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>04adr</th>\n",
       "      <th>...</th>\n",
       "      <th>研究の進め方</th>\n",
       "      <th>結合タスク自己教師あり学習や音楽からダンスへの生成プロセスに関する5つの論文を発表しました</th>\n",
       "      <th>詳しくはこちらをご覧ください</th>\n",
       "      <th>論文の書き方まで説明してある</th>\n",
       "      <th>详解图的机器学习趋势</th>\n",
       "      <th>長蛇の列に</th>\n",
       "      <th>雷锋网</th>\n",
       "      <th>音楽</th>\n",
       "      <th>齋藤さん</th>\n",
       "      <th>ﾎﾎｩ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.019</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 11108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                00    000  000mph   00pm     01     02  02476     03     04  \\\n",
       "component_1  0.019  0.005     0.0  0.004  0.001  0.003  0.001  0.001  0.003   \n",
       "component_2 -0.034 -0.004    -0.0 -0.007 -0.002  0.001  0.002  0.000 -0.001   \n",
       "\n",
       "             04adr  ...  研究の進め方  \\\n",
       "component_1    0.0  ...     0.0   \n",
       "component_2   -0.0  ...     0.0   \n",
       "\n",
       "             結合タスク自己教師あり学習や音楽からダンスへの生成プロセスに関する5つの論文を発表しました  詳しくはこちらをご覧ください  \\\n",
       "component_1                                            0.0             0.0   \n",
       "component_2                                            0.0             0.0   \n",
       "\n",
       "             論文の書き方まで説明してある  详解图的机器学习趋势  長蛇の列に  雷锋网     音楽   齋藤さん  ﾎﾎｩ  \n",
       "component_1             0.0        -0.0    0.0 -0.0  0.000  0.000  0.0  \n",
       "component_2             0.0        -0.0    0.0 -0.0  0.001  0.001 -0.0  \n",
       "\n",
       "[2 rows x 11108 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\"],\n",
    "             columns = vectorizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "learning, workshop, ai, poster, deep\n",
      "\n",
      "Topic  1\n",
      "learning, deep, machine, bengio, yoshua\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, vectorizer.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean_tweet</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>former researcher phd shared highlights from his neurips2019 paper geometry aware neural rendering in the exclusive interview with</th>\n",
       "      <td>0.23613</td>\n",
       "      <td>-0.06739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what happens to accuracy when we try to increase fairness in machinelearning a paper presented at neurips2019 has the answer</th>\n",
       "      <td>0.27252</td>\n",
       "      <td>-0.10808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oracle how can you improve both privacy and accuracy when training machinelearning models learn what differentially private federated learning is in this blog post based on a neurips2019 paper</th>\n",
       "      <td>0.99299</td>\n",
       "      <td>0.51753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today we’re joined by ilias diakonikolas faculty at the and author of the paper distribution independent pac learning of halfspaces with massart noise recipient of the neurips2019 outstanding paper award</th>\n",
       "      <td>1.01388</td>\n",
       "      <td>0.30867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shared his latest research in deeplearning theory at neurips2019 the work behind his acm doctoral dissertation honorablemention award insights on the future of ai research view</th>\n",
       "      <td>0.77206</td>\n",
       "      <td>-0.59349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thanks to av technology presenter who arent able to attend due to denies visas can present remotely with great audio visual quality thank you organizers for making this happen neurips2019</th>\n",
       "      <td>0.27069</td>\n",
       "      <td>-0.20475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>erick tornero talking about a reinforcement learning approach to fly quadcopters with a faulted rotor neurips2019</th>\n",
       "      <td>0.64867</td>\n",
       "      <td>0.67233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran out of he him pronoun stickers pretty accurate gender representation neurips2019</th>\n",
       "      <td>0.03625</td>\n",
       "      <td>0.01366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural architecture search is promising but expensive the work intrigues me per links to evolution of nervous systems petridish uses forward search to generate architectures from small parent models neurips2019</th>\n",
       "      <td>0.39839</td>\n",
       "      <td>-0.19704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if you are attending neurips2019 in vancouver you can find and from lab who will be presenting our work on the impact of gender bias in medical imaging classifiers for computer aided diagnosis in the workshop</th>\n",
       "      <td>0.73557</td>\n",
       "      <td>-0.35895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15299 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    component_1  component_2\n",
       "clean_tweet                                                                 \n",
       "former researcher phd shared highlights from hi...      0.23613     -0.06739\n",
       "what happens to accuracy when we try to increas...      0.27252     -0.10808\n",
       "oracle how can you improve both privacy and acc...      0.99299      0.51753\n",
       "today we’re joined by ilias diakonikolas facult...      1.01388      0.30867\n",
       "shared his latest research in deeplearning theo...      0.77206     -0.59349\n",
       "...                                                         ...          ...\n",
       "thanks to av technology presenter who arent abl...      0.27069     -0.20475\n",
       "erick tornero talking about a reinforcement lea...      0.64867      0.67233\n",
       "ran out of he him pronoun stickers pretty accur...      0.03625      0.01366\n",
       "neural architecture search is promising but exp...      0.39839     -0.19704\n",
       "if you are attending neurips2019 in vancouver y...      0.73557     -0.35895\n",
       "\n",
       "[15299 rows x 2 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vt = pd.DataFrame(doc_topic.round(5),\n",
    "             index = corpus,\n",
    "             columns = [\"component_1\",\"component_2\" ])\n",
    "Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.99504758, 0.72590048, ..., 0.80306505, 0.98360874,\n",
       "        0.98455378],\n",
       "       [0.99504758, 1.        , 0.65393838, ..., 0.73985647, 0.99666086,\n",
       "        0.99708101],\n",
       "       [0.72590048, 0.65393838, 1.        , ..., 0.99279924, 0.5899808 ,\n",
       "        0.59426643],\n",
       "       ...,\n",
       "       [0.80306505, 0.73985647, 0.99279924, ..., 1.        , 0.68245309,\n",
       "        0.68633064],\n",
       "       [0.98360874, 0.99666086, 0.5899808 , ..., 0.68245309, 1.        ,\n",
       "        0.99998586],\n",
       "       [0.98455378, 0.99708101, 0.59426643, ..., 0.68633064, 0.99998586,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.1 , 0.08, 0.13, 0.07, 0.  , 0.  , 0.08, 0.  , 0.  ],\n",
       "       [0.1 , 1.  , 0.25, 0.15, 0.  , 0.  , 0.08, 0.26, 0.  , 0.  ],\n",
       "       [0.08, 0.25, 1.  , 0.17, 0.  , 0.  , 0.06, 0.97, 0.  , 0.  ],\n",
       "       [0.13, 0.15, 0.17, 1.  , 0.05, 0.  , 0.  , 0.17, 0.  , 0.  ],\n",
       "       [0.07, 0.  , 0.  , 0.05, 1.  , 0.  , 0.11, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.08, 0.06, 0.  , 0.11, 0.  , 1.  , 0.06, 0.  , 0.  ],\n",
       "       [0.08, 0.26, 0.97, 0.17, 0.  , 0.  , 0.06, 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  ]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_dtm = cosine_similarity(dtm_lsa).round(2)\n",
    "cos_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_dtm[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_dtm[0][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(n_components=5)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 5 topics:\n",
    "nmf = NMF(n_components=5)\n",
    "nmf.fit(tweet_word_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet/Topic Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15299, 5)\n",
      "(15299, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.011145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.006048</td>\n",
       "      <td>@josh_tobin_ , former researcher @OpenAI &amp;amp;...</td>\n",
       "      <td>former researcher phd shared highlights from h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.086019</td>\n",
       "      <td>What happens to accuracy when we try to increa...</td>\n",
       "      <td>what happens to accuracy when we try to increa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.069486</td>\n",
       "      <td>Oracle: How can you improve both privacy and a...</td>\n",
       "      <td>oracle how can you improve both privacy and ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.024172</td>\n",
       "      <td>0.006175</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>Today we’re joined by Ilias Diakonikolas, facu...</td>\n",
       "      <td>today we’re joined by ilias diakonikolas facul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.017927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048573</td>\n",
       "      <td>@tengyuma @StanfordAILab @Stanford shared his ...</td>\n",
       "      <td>shared his latest research in deeplearning the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_0   topic_1   topic_2   topic_3   topic_4  \\\n",
       "0  0.000011  0.011145  0.000000  0.001420  0.006048   \n",
       "1  0.000000  0.006707  0.000000  0.002365  0.086019   \n",
       "2  0.000000  0.029209  0.000000  0.000320  0.069486   \n",
       "3  0.000042  0.024172  0.006175  0.003147  0.001461   \n",
       "4  0.000031  0.017927  0.000000  0.000000  0.048573   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @josh_tobin_ , former researcher @OpenAI &amp;...   \n",
       "1  What happens to accuracy when we try to increa...   \n",
       "2  Oracle: How can you improve both privacy and a...   \n",
       "3  Today we’re joined by Ilias Diakonikolas, facu...   \n",
       "4  @tengyuma @StanfordAILab @Stanford shared his ...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  former researcher phd shared highlights from h...  \n",
       "1  what happens to accuracy when we try to increa...  \n",
       "2  oracle how can you improve both privacy and ac...  \n",
       "3  today we’re joined by ilias diakonikolas facul...  \n",
       "4  shared his latest research in deeplearning the...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_topic_matrix = nmf.transform(tweet_word_matrix)\n",
    "tweet_topic_matrix_df = pd.DataFrame(tweet_topic_matrix).add_prefix('topic_')\n",
    "\n",
    "tweet_topic_matrix_df = pd.concat([tweet_topic_matrix_df.reset_index(drop=True), df_2019[['tweet', 'clean_tweet']].reset_index(drop=True)], axis=1)\n",
    "tweet_topic_matrix_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word/Topic Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035743</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.088625</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000mph</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00pm</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.022943</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic_0   topic_1   topic_2   topic_3   topic_4\n",
       "00          0.0  0.035743  0.002021  0.088625  0.000000\n",
       "000         0.0  0.014926  0.000000  0.000000  0.017033\n",
       "000mph      0.0  0.000467  0.000000  0.000050  0.000000\n",
       "00pm        0.0  0.005736  0.001326  0.022943  0.000000\n",
       "01          0.0  0.001735  0.000000  0.006525  0.000000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_topic_matrix_df = pd.DataFrame(nmf.components_, columns=vocab).T.add_prefix('topic_')\n",
    "word_topic_matrix_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Interpretation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9731</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315025</td>\n",
       "      <td>#NeurIPS2019 #MachineLearning https://t.co/u6E...</td>\n",
       "      <td>neurips2019 machinelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8875</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315025</td>\n",
       "      <td>#NeurIPS2019 #MachineLearning https://t.co/PzW...</td>\n",
       "      <td>neurips2019 machinelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11939</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315025</td>\n",
       "      <td>#NeurIPS2019 #MachineLearning https://t.co/RRO...</td>\n",
       "      <td>neurips2019 machinelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315025</td>\n",
       "      <td>#NeurIPS2019 #MachineLearning https://t.co/u6E...</td>\n",
       "      <td>neurips2019 machinelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315025</td>\n",
       "      <td>#NeurIPS2019 #MachineLearning https://t.co/scm...</td>\n",
       "      <td>neurips2019 machinelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7575</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315025</td>\n",
       "      <td>#NeurIPS2019 #MachineLearning https://t.co/sii...</td>\n",
       "      <td>neurips2019 machinelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8212</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315025</td>\n",
       "      <td>#NeurIPS2019 #MachineLearning https://t.co/Opl...</td>\n",
       "      <td>neurips2019 machinelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13918</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315025</td>\n",
       "      <td>#NeurIPS2019 #MachineLearning https://t.co/MW1...</td>\n",
       "      <td>neurips2019 machinelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315025</td>\n",
       "      <td>#NeurIPS2019 #MachineLearning https://t.co/56S...</td>\n",
       "      <td>neurips2019 machinelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315025</td>\n",
       "      <td>#NeurIPS2019 #MachineLearning https://t.co/scm...</td>\n",
       "      <td>neurips2019 machinelearning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic_0  topic_1  topic_2  topic_3   topic_4  \\\n",
       "9731       0.0      0.0      0.0      0.0  0.315025   \n",
       "8875       0.0      0.0      0.0      0.0  0.315025   \n",
       "11939      0.0      0.0      0.0      0.0  0.315025   \n",
       "4631       0.0      0.0      0.0      0.0  0.315025   \n",
       "2801       0.0      0.0      0.0      0.0  0.315025   \n",
       "7575       0.0      0.0      0.0      0.0  0.315025   \n",
       "8212       0.0      0.0      0.0      0.0  0.315025   \n",
       "13918      0.0      0.0      0.0      0.0  0.315025   \n",
       "1254       0.0      0.0      0.0      0.0  0.315025   \n",
       "13001      0.0      0.0      0.0      0.0  0.315025   \n",
       "\n",
       "                                                   tweet  \\\n",
       "9731   #NeurIPS2019 #MachineLearning https://t.co/u6E...   \n",
       "8875   #NeurIPS2019 #MachineLearning https://t.co/PzW...   \n",
       "11939  #NeurIPS2019 #MachineLearning https://t.co/RRO...   \n",
       "4631   #NeurIPS2019 #MachineLearning https://t.co/u6E...   \n",
       "2801   #NeurIPS2019 #MachineLearning https://t.co/scm...   \n",
       "7575   #NeurIPS2019 #MachineLearning https://t.co/sii...   \n",
       "8212   #NeurIPS2019 #MachineLearning https://t.co/Opl...   \n",
       "13918  #NeurIPS2019 #MachineLearning https://t.co/MW1...   \n",
       "1254   #NeurIPS2019 #MachineLearning https://t.co/56S...   \n",
       "13001  #NeurIPS2019 #MachineLearning https://t.co/scm...   \n",
       "\n",
       "                        clean_tweet  \n",
       "9731   neurips2019 machinelearning   \n",
       "8875   neurips2019 machinelearning   \n",
       "11939  neurips2019 machinelearning   \n",
       "4631   neurips2019 machinelearning   \n",
       "2801   neurips2019 machinelearning   \n",
       "7575   neurips2019 machinelearning   \n",
       "8212   neurips2019 machinelearning   \n",
       "13918  neurips2019 machinelearning   \n",
       "1254   neurips2019 machinelearning   \n",
       "13001  neurips2019 machinelearning   "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_topic_matrix_df.sort_values(by='topic_4', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the topics are as follows:\n",
    "\n",
    "1. topic_0 = invalid tweet\n",
    "2. topic_1 = Yoshua Bengio\n",
    "3. topic_2 = RNAbench2bedside\n",
    "4. topic_3 = poster session links\n",
    "5. topic_4 = tweets or retweets to other tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(ngram_range=(1, 2), stop_words='english',\n",
       "                token_pattern='\\\\b[a-z][a-z]+\\\\b')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", max_df = 0.50)\n",
    "\n",
    "count_vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15289</th>\n",
       "      <th>15290</th>\n",
       "      <th>15291</th>\n",
       "      <th>15292</th>\n",
       "      <th>15293</th>\n",
       "      <th>15294</th>\n",
       "      <th>15295</th>\n",
       "      <th>15296</th>\n",
       "      <th>15297</th>\n",
       "      <th>15298</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa woman</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaaaand</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaaaand fear</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaand</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 15299 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0      1      2      3      4      5      6      7      8      \\\n",
       "aa                  0      0      0      0      0      0      0      0      0   \n",
       "aa woman            0      0      0      0      0      0      0      0      0   \n",
       "aaaaaaand           0      0      0      0      0      0      0      0      0   \n",
       "aaaaaaand fear      0      0      0      0      0      0      0      0      0   \n",
       "aaaaand             0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "                9      ...  15289  15290  15291  15292  15293  15294  15295  \\\n",
       "aa                  0  ...      0      0      0      0      0      0      0   \n",
       "aa woman            0  ...      0      0      0      0      0      0      0   \n",
       "aaaaaaand           0  ...      0      0      0      0      0      0      0   \n",
       "aaaaaaand fear      0  ...      0      0      0      0      0      0      0   \n",
       "aaaaand             0  ...      0      0      0      0      0      0      0   \n",
       "\n",
       "                15296  15297  15298  \n",
       "aa                  0      0      0  \n",
       "aa woman            0      0      0  \n",
       "aaaaaaand           0      0      0  \n",
       "aaaaaaand fear      0      0      0  \n",
       "aaaaand             0      0      0  \n",
       "\n",
       "[5 rows x 15299 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word = count_vectorizer.transform(corpus).transpose()\n",
    "pd.DataFrame(doc_word.toarray(), count_vectorizer.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50350"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())\n",
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ROGRESS: pass 2, at document #6000/15299\n",
      "2020-11-08 20:13:50,918 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:50,928 : INFO : topic #0 (0.333): 0.008*\"poster\" + 0.006*\"learning\" + 0.005*\"neurips\" + 0.005*\"work\" + 0.005*\"workshop\" + 0.004*\"today\" + 0.004*\"ai\" + 0.004*\"come\" + 0.003*\"paper\" + 0.003*\"research\"\n",
      "2020-11-08 20:13:50,929 : INFO : topic #1 (0.333): 0.012*\"learning\" + 0.007*\"talk\" + 0.006*\"ai\" + 0.006*\"workshop\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"deep learning\" + 0.003*\"machine\" + 0.003*\"bengio\"\n",
      "2020-11-08 20:13:50,931 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"poster\" + 0.004*\"great\" + 0.004*\"vancouver\" + 0.003*\"work\" + 0.003*\"workshop\" + 0.003*\"paper\" + 0.003*\"tweet\" + 0.003*\"talk\" + 0.003*\"time\"\n",
      "2020-11-08 20:13:50,932 : INFO : topic diff=0.175528, rho=0.306433\n",
      "2020-11-08 20:13:50,959 : INFO : PROGRESS: pass 2, at document #8000/15299\n",
      "2020-11-08 20:13:51,493 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:51,502 : INFO : topic #0 (0.333): 0.008*\"poster\" + 0.007*\"workshop\" + 0.006*\"learning\" + 0.005*\"work\" + 0.005*\"neurips\" + 0.004*\"today\" + 0.004*\"ai\" + 0.004*\"paper\" + 0.004*\"come\" + 0.003*\"research\"\n",
      "2020-11-08 20:13:51,505 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.008*\"workshop\" + 0.007*\"talk\" + 0.006*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"machine\" + 0.003*\"machine learning\" + 0.003*\"deep learning\"\n",
      "2020-11-08 20:13:51,506 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"workshop\" + 0.004*\"poster\" + 0.004*\"great\" + 0.003*\"work\" + 0.003*\"vancouver\" + 0.003*\"paper\" + 0.003*\"tweet\" + 0.003*\"talk\" + 0.003*\"ml\"\n",
      "2020-11-08 20:13:51,508 : INFO : topic diff=0.185618, rho=0.306433\n",
      "2020-11-08 20:13:51,530 : INFO : PROGRESS: pass 2, at document #10000/15299\n",
      "2020-11-08 20:13:51,981 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:51,991 : INFO : topic #0 (0.333): 0.009*\"poster\" + 0.006*\"learning\" + 0.005*\"workshop\" + 0.005*\"work\" + 0.005*\"today\" + 0.005*\"neurips\" + 0.004*\"come\" + 0.004*\"ai\" + 0.004*\"hall\" + 0.004*\"paper\"\n",
      "2020-11-08 20:13:51,993 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.007*\"talk\" + 0.006*\"workshop\" + 0.005*\"ai\" + 0.005*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"bengio\" + 0.003*\"deep learning\" + 0.003*\"machine\"\n",
      "2020-11-08 20:13:51,995 : INFO : topic #2 (0.333): 0.006*\"ai\" + 0.004*\"poster\" + 0.004*\"vancouver\" + 0.003*\"great\" + 0.003*\"work\" + 0.003*\"workshop\" + 0.003*\"tweet\" + 0.003*\"talk\" + 0.003*\"paper\" + 0.002*\"ml\"\n",
      "2020-11-08 20:13:51,996 : INFO : topic diff=0.174278, rho=0.306433\n",
      "2020-11-08 20:13:52,018 : INFO : PROGRESS: pass 2, at document #12000/15299\n",
      "2020-11-08 20:13:52,554 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:52,564 : INFO : topic #0 (0.333): 0.008*\"poster\" + 0.006*\"learning\" + 0.006*\"workshop\" + 0.005*\"work\" + 0.005*\"neurips\" + 0.004*\"ai\" + 0.004*\"today\" + 0.004*\"research\" + 0.003*\"paper\" + 0.003*\"come\"\n",
      "2020-11-08 20:13:52,566 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.007*\"workshop\" + 0.007*\"talk\" + 0.006*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"machine\" + 0.003*\"deep learning\" + 0.003*\"machine learning\"\n",
      "2020-11-08 20:13:52,574 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"great\" + 0.004*\"workshop\" + 0.004*\"poster\" + 0.003*\"vancouver\" + 0.003*\"work\" + 0.003*\"tweet\" + 0.003*\"paper\" + 0.003*\"talk\" + 0.003*\"ml\"\n",
      "2020-11-08 20:13:52,579 : INFO : topic diff=0.186876, rho=0.306433\n",
      "2020-11-08 20:13:52,610 : INFO : PROGRESS: pass 2, at document #14000/15299\n",
      "2020-11-08 20:13:53,105 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:53,115 : INFO : topic #0 (0.333): 0.009*\"poster\" + 0.006*\"learning\" + 0.006*\"workshop\" + 0.005*\"work\" + 0.005*\"neurips\" + 0.004*\"today\" + 0.004*\"ai\" + 0.004*\"come\" + 0.004*\"paper\" + 0.003*\"research\"\n",
      "2020-11-08 20:13:53,117 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.007*\"talk\" + 0.007*\"workshop\" + 0.006*\"ai\" + 0.005*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.004*\"bengio\" + 0.004*\"deep learning\" + 0.003*\"machine\"\n",
      "2020-11-08 20:13:53,123 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"poster\" + 0.003*\"great\" + 0.003*\"work\" + 0.003*\"workshop\" + 0.003*\"vancouver\" + 0.003*\"paper\" + 0.003*\"talk\" + 0.003*\"tweet\" + 0.003*\"ml\"\n",
      "2020-11-08 20:13:53,124 : INFO : topic diff=0.194202, rho=0.306433\n",
      "2020-11-08 20:13:53,718 : INFO : -9.885 per-word bound, 945.7 perplexity estimate based on a held-out corpus of 1299 documents with 27022 words\n",
      "2020-11-08 20:13:53,719 : INFO : PROGRESS: pass 2, at document #15299/15299\n",
      "2020-11-08 20:13:54,036 : INFO : merging changes from 1299 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:54,045 : INFO : topic #0 (0.333): 0.009*\"poster\" + 0.006*\"learning\" + 0.005*\"work\" + 0.005*\"workshop\" + 0.005*\"today\" + 0.005*\"neurips\" + 0.004*\"come\" + 0.004*\"hall\" + 0.004*\"ai\" + 0.003*\"paper\"\n",
      "2020-11-08 20:13:54,047 : INFO : topic #1 (0.333): 0.012*\"learning\" + 0.007*\"talk\" + 0.005*\"workshop\" + 0.005*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"deep learning\" + 0.003*\"machine\" + 0.003*\"bengio\"\n",
      "2020-11-08 20:13:54,049 : INFO : topic #2 (0.333): 0.006*\"ai\" + 0.004*\"poster\" + 0.004*\"vancouver\" + 0.004*\"great\" + 0.003*\"work\" + 0.003*\"tweet\" + 0.003*\"workshop\" + 0.003*\"talk\" + 0.003*\"paper\" + 0.002*\"ml\"\n",
      "2020-11-08 20:13:54,050 : INFO : topic diff=0.201160, rho=0.306433\n",
      "2020-11-08 20:13:54,078 : INFO : PROGRESS: pass 3, at document #2000/15299\n",
      "2020-11-08 20:13:54,609 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:54,617 : INFO : topic #0 (0.333): 0.008*\"poster\" + 0.006*\"learning\" + 0.006*\"workshop\" + 0.005*\"neurips\" + 0.005*\"work\" + 0.004*\"ai\" + 0.004*\"today\" + 0.004*\"paper\" + 0.003*\"research\" + 0.003*\"come\"\n",
      "2020-11-08 20:13:54,619 : INFO : topic #1 (0.333): 0.012*\"learning\" + 0.007*\"workshop\" + 0.007*\"talk\" + 0.006*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"machine\" + 0.003*\"machine learning\" + 0.003*\"deep learning\"\n",
      "2020-11-08 20:13:54,621 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"great\" + 0.004*\"workshop\" + 0.004*\"poster\" + 0.003*\"vancouver\" + 0.003*\"work\" + 0.003*\"tweet\" + 0.003*\"paper\" + 0.003*\"talk\" + 0.003*\"ml\"\n",
      "2020-11-08 20:13:54,624 : INFO : topic diff=0.190087, rho=0.292986\n",
      "2020-11-08 20:13:54,646 : INFO : PROGRESS: pass 3, at document #4000/15299\n",
      "2020-11-08 20:13:55,115 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:55,124 : INFO : topic #0 (0.333): 0.009*\"poster\" + 0.006*\"learning\" + 0.006*\"workshop\" + 0.005*\"work\" + 0.005*\"neurips\" + 0.005*\"today\" + 0.004*\"ai\" + 0.004*\"come\" + 0.004*\"paper\" + 0.003*\"hall\"\n",
      "2020-11-08 20:13:55,126 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.007*\"talk\" + 0.007*\"workshop\" + 0.006*\"ai\" + 0.005*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.004*\"deep learning\" + 0.004*\"bengio\" + 0.003*\"machine\"\n",
      "2020-11-08 20:13:55,128 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"poster\" + 0.003*\"great\" + 0.003*\"work\" + 0.003*\"vancouver\" + 0.003*\"workshop\" + 0.003*\"paper\" + 0.003*\"tweet\" + 0.003*\"talk\" + 0.003*\"ml\"\n",
      "2020-11-08 20:13:55,129 : INFO : topic diff=0.196275, rho=0.292986\n",
      "2020-11-08 20:13:55,150 : INFO : PROGRESS: pass 3, at document #6000/15299\n",
      "2020-11-08 20:13:55,595 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:55,604 : INFO : topic #0 (0.333): 0.008*\"poster\" + 0.006*\"learning\" + 0.005*\"neurips\" + 0.005*\"work\" + 0.005*\"workshop\" + 0.004*\"today\" + 0.004*\"ai\" + 0.004*\"come\" + 0.003*\"paper\" + 0.003*\"research\"\n",
      "2020-11-08 20:13:55,606 : INFO : topic #1 (0.333): 0.012*\"learning\" + 0.007*\"talk\" + 0.006*\"ai\" + 0.006*\"workshop\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"deep learning\" + 0.003*\"machine\" + 0.003*\"bengio\"\n",
      "2020-11-08 20:13:55,608 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"poster\" + 0.004*\"great\" + 0.004*\"vancouver\" + 0.003*\"work\" + 0.003*\"workshop\" + 0.003*\"paper\" + 0.003*\"tweet\" + 0.003*\"talk\" + 0.003*\"time\"\n",
      "2020-11-08 20:13:55,609 : INFO : topic diff=0.164018, rho=0.292986\n",
      "2020-11-08 20:13:55,631 : INFO : PROGRESS: pass 3, at document #8000/15299\n",
      "2020-11-08 20:13:56,105 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:56,114 : INFO : topic #0 (0.333): 0.008*\"poster\" + 0.007*\"workshop\" + 0.006*\"learning\" + 0.005*\"work\" + 0.005*\"neurips\" + 0.004*\"today\" + 0.004*\"ai\" + 0.004*\"come\" + 0.004*\"paper\" + 0.003*\"research\"\n",
      "2020-11-08 20:13:56,116 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.008*\"workshop\" + 0.007*\"talk\" + 0.006*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"machine\" + 0.003*\"machine learning\" + 0.003*\"deep learning\"\n",
      "2020-11-08 20:13:56,118 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"poster\" + 0.004*\"workshop\" + 0.004*\"great\" + 0.003*\"work\" + 0.003*\"vancouver\" + 0.003*\"paper\" + 0.003*\"tweet\" + 0.003*\"talk\" + 0.003*\"ml\"\n",
      "2020-11-08 20:13:56,120 : INFO : topic diff=0.173049, rho=0.292986\n",
      "2020-11-08 20:13:56,141 : INFO : PROGRESS: pass 3, at document #10000/15299\n",
      "2020-11-08 20:13:56,603 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:56,611 : INFO : topic #0 (0.333): 0.009*\"poster\" + 0.006*\"learning\" + 0.005*\"workshop\" + 0.005*\"work\" + 0.005*\"today\" + 0.005*\"neurips\" + 0.004*\"come\" + 0.004*\"ai\" + 0.004*\"hall\" + 0.003*\"paper\"\n",
      "2020-11-08 20:13:56,612 : INFO : topic #1 (0.333): 0.012*\"learning\" + 0.007*\"talk\" + 0.006*\"workshop\" + 0.005*\"ai\" + 0.005*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"bengio\" + 0.003*\"deep learning\" + 0.003*\"machine\"\n",
      "2020-11-08 20:13:56,614 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"poster\" + 0.004*\"vancouver\" + 0.003*\"great\" + 0.003*\"work\" + 0.003*\"workshop\" + 0.003*\"tweet\" + 0.003*\"talk\" + 0.003*\"paper\" + 0.002*\"ml\"\n",
      "2020-11-08 20:13:56,615 : INFO : topic diff=0.163995, rho=0.292986\n",
      "2020-11-08 20:13:56,637 : INFO : PROGRESS: pass 3, at document #12000/15299\n",
      "2020-11-08 20:13:57,130 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:57,139 : INFO : topic #0 (0.333): 0.008*\"poster\" + 0.006*\"learning\" + 0.006*\"workshop\" + 0.005*\"work\" + 0.005*\"neurips\" + 0.004*\"ai\" + 0.004*\"today\" + 0.004*\"research\" + 0.003*\"paper\" + 0.003*\"come\"\n",
      "2020-11-08 20:13:57,141 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.007*\"workshop\" + 0.007*\"talk\" + 0.006*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"machine\" + 0.003*\"deep learning\" + 0.003*\"machine learning\"\n",
      "2020-11-08 20:13:57,144 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"great\" + 0.004*\"poster\" + 0.004*\"workshop\" + 0.003*\"vancouver\" + 0.003*\"work\" + 0.003*\"tweet\" + 0.003*\"paper\" + 0.003*\"talk\" + 0.003*\"ml\"\n",
      "2020-11-08 20:13:57,145 : INFO : topic diff=0.175744, rho=0.292986\n",
      "2020-11-08 20:13:57,178 : INFO : PROGRESS: pass 3, at document #14000/15299\n",
      "2020-11-08 20:13:57,639 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:57,647 : INFO : topic #0 (0.333): 0.009*\"poster\" + 0.006*\"learning\" + 0.006*\"workshop\" + 0.005*\"work\" + 0.005*\"neurips\" + 0.004*\"today\" + 0.004*\"ai\" + 0.004*\"come\" + 0.004*\"paper\" + 0.003*\"research\"\n",
      "2020-11-08 20:13:57,650 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.007*\"talk\" + 0.007*\"workshop\" + 0.006*\"ai\" + 0.005*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.004*\"bengio\" + 0.004*\"deep learning\" + 0.003*\"machine\"\n",
      "2020-11-08 20:13:57,653 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"poster\" + 0.003*\"great\" + 0.003*\"work\" + 0.003*\"workshop\" + 0.003*\"vancouver\" + 0.003*\"paper\" + 0.003*\"talk\" + 0.003*\"tweet\" + 0.003*\"ml\"\n",
      "2020-11-08 20:13:57,655 : INFO : topic diff=0.183621, rho=0.292986\n",
      "2020-11-08 20:13:58,308 : INFO : -9.866 per-word bound, 933.0 perplexity estimate based on a held-out corpus of 1299 documents with 27022 words\n",
      "2020-11-08 20:13:58,309 : INFO : PROGRESS: pass 3, at document #15299/15299\n",
      "2020-11-08 20:13:58,641 : INFO : merging changes from 1299 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:58,650 : INFO : topic #0 (0.333): 0.009*\"poster\" + 0.006*\"learning\" + 0.005*\"work\" + 0.005*\"workshop\" + 0.005*\"today\" + 0.005*\"neurips\" + 0.004*\"come\" + 0.004*\"hall\" + 0.004*\"ai\" + 0.003*\"paper\"\n",
      "2020-11-08 20:13:58,652 : INFO : topic #1 (0.333): 0.012*\"learning\" + 0.007*\"talk\" + 0.005*\"workshop\" + 0.005*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"deep learning\" + 0.003*\"machine\" + 0.003*\"bengio\"\n",
      "2020-11-08 20:13:58,655 : INFO : topic #2 (0.333): 0.006*\"ai\" + 0.004*\"poster\" + 0.004*\"vancouver\" + 0.004*\"great\" + 0.003*\"work\" + 0.003*\"tweet\" + 0.003*\"workshop\" + 0.003*\"talk\" + 0.003*\"paper\" + 0.002*\"ml\"\n",
      "2020-11-08 20:13:58,656 : INFO : topic diff=0.189739, rho=0.292986\n",
      "2020-11-08 20:13:58,681 : INFO : PROGRESS: pass 4, at document #2000/15299\n",
      "2020-11-08 20:13:59,181 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:59,191 : INFO : topic #0 (0.333): 0.008*\"poster\" + 0.006*\"learning\" + 0.006*\"workshop\" + 0.005*\"neurips\" + 0.005*\"work\" + 0.004*\"ai\" + 0.004*\"today\" + 0.004*\"paper\" + 0.003*\"research\" + 0.003*\"come\"\n",
      "2020-11-08 20:13:59,193 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.007*\"workshop\" + 0.007*\"talk\" + 0.006*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"machine\" + 0.003*\"machine learning\" + 0.003*\"deep learning\"\n",
      "2020-11-08 20:13:59,194 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"great\" + 0.004*\"poster\" + 0.004*\"workshop\" + 0.003*\"vancouver\" + 0.003*\"work\" + 0.003*\"tweet\" + 0.003*\"paper\" + 0.003*\"talk\" + 0.003*\"ml\"\n",
      "2020-11-08 20:13:59,195 : INFO : topic diff=0.179733, rho=0.281166\n",
      "2020-11-08 20:13:59,228 : INFO : PROGRESS: pass 4, at document #4000/15299\n",
      "2020-11-08 20:13:59,784 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:13:59,793 : INFO : topic #0 (0.333): 0.009*\"poster\" + 0.006*\"learning\" + 0.006*\"workshop\" + 0.005*\"work\" + 0.005*\"neurips\" + 0.005*\"today\" + 0.004*\"ai\" + 0.004*\"come\" + 0.004*\"paper\" + 0.003*\"hall\"\n",
      "2020-11-08 20:13:59,796 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.007*\"talk\" + 0.007*\"workshop\" + 0.006*\"ai\" + 0.005*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.004*\"deep learning\" + 0.004*\"bengio\" + 0.003*\"machine\"\n",
      "2020-11-08 20:13:59,799 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"poster\" + 0.003*\"great\" + 0.003*\"work\" + 0.003*\"vancouver\" + 0.003*\"workshop\" + 0.003*\"paper\" + 0.003*\"tweet\" + 0.003*\"talk\" + 0.003*\"ml\"\n",
      "2020-11-08 20:13:59,801 : INFO : topic diff=0.186333, rho=0.281166\n",
      "2020-11-08 20:13:59,950 : INFO : PROGRESS: pass 4, at document #6000/15299\n",
      "2020-11-08 20:14:00,410 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:14:00,418 : INFO : topic #0 (0.333): 0.008*\"poster\" + 0.006*\"learning\" + 0.005*\"work\" + 0.005*\"neurips\" + 0.005*\"workshop\" + 0.004*\"today\" + 0.004*\"ai\" + 0.004*\"come\" + 0.003*\"paper\" + 0.003*\"research\"\n",
      "2020-11-08 20:14:00,421 : INFO : topic #1 (0.333): 0.012*\"learning\" + 0.007*\"talk\" + 0.006*\"ai\" + 0.006*\"workshop\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"deep learning\" + 0.003*\"machine\" + 0.003*\"bengio\"\n",
      "2020-11-08 20:14:00,423 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"poster\" + 0.004*\"great\" + 0.004*\"vancouver\" + 0.003*\"work\" + 0.003*\"workshop\" + 0.003*\"paper\" + 0.003*\"tweet\" + 0.003*\"talk\" + 0.003*\"time\"\n",
      "2020-11-08 20:14:00,424 : INFO : topic diff=0.155788, rho=0.281166\n",
      "2020-11-08 20:14:00,443 : INFO : PROGRESS: pass 4, at document #8000/15299\n",
      "2020-11-08 20:14:01,022 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:14:01,029 : INFO : topic #0 (0.333): 0.008*\"poster\" + 0.007*\"workshop\" + 0.006*\"learning\" + 0.005*\"work\" + 0.005*\"neurips\" + 0.004*\"today\" + 0.004*\"ai\" + 0.004*\"come\" + 0.004*\"paper\" + 0.003*\"research\"\n",
      "2020-11-08 20:14:01,031 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.008*\"workshop\" + 0.007*\"talk\" + 0.006*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"machine\" + 0.003*\"machine learning\" + 0.003*\"deep learning\"\n",
      "2020-11-08 20:14:01,033 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"poster\" + 0.004*\"workshop\" + 0.004*\"great\" + 0.003*\"work\" + 0.003*\"vancouver\" + 0.003*\"paper\" + 0.003*\"tweet\" + 0.003*\"talk\" + 0.003*\"ml\"\n",
      "2020-11-08 20:14:01,037 : INFO : topic diff=0.164340, rho=0.281166\n",
      "2020-11-08 20:14:01,059 : INFO : PROGRESS: pass 4, at document #10000/15299\n",
      "2020-11-08 20:14:01,478 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:14:01,487 : INFO : topic #0 (0.333): 0.009*\"poster\" + 0.006*\"learning\" + 0.005*\"workshop\" + 0.005*\"work\" + 0.005*\"today\" + 0.005*\"neurips\" + 0.004*\"come\" + 0.004*\"ai\" + 0.004*\"hall\" + 0.003*\"paper\"\n",
      "2020-11-08 20:14:01,489 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.007*\"talk\" + 0.006*\"workshop\" + 0.005*\"ai\" + 0.005*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"bengio\" + 0.003*\"deep learning\" + 0.003*\"machine\"\n",
      "2020-11-08 20:14:01,491 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"poster\" + 0.004*\"vancouver\" + 0.003*\"great\" + 0.003*\"work\" + 0.003*\"workshop\" + 0.003*\"tweet\" + 0.003*\"talk\" + 0.003*\"paper\" + 0.002*\"ml\"\n",
      "2020-11-08 20:14:01,492 : INFO : topic diff=0.155707, rho=0.281166\n",
      "2020-11-08 20:14:01,513 : INFO : PROGRESS: pass 4, at document #12000/15299\n",
      "2020-11-08 20:14:02,034 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:14:02,044 : INFO : topic #0 (0.333): 0.008*\"poster\" + 0.006*\"learning\" + 0.006*\"workshop\" + 0.005*\"work\" + 0.005*\"neurips\" + 0.004*\"ai\" + 0.004*\"today\" + 0.004*\"research\" + 0.003*\"paper\" + 0.003*\"come\"\n",
      "2020-11-08 20:14:02,045 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.007*\"workshop\" + 0.007*\"talk\" + 0.006*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"machine\" + 0.003*\"deep learning\" + 0.003*\"machine learning\"\n",
      "2020-11-08 20:14:02,047 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"great\" + 0.004*\"poster\" + 0.004*\"workshop\" + 0.003*\"vancouver\" + 0.003*\"work\" + 0.003*\"paper\" + 0.003*\"tweet\" + 0.003*\"talk\" + 0.003*\"ml\"\n",
      "2020-11-08 20:14:02,048 : INFO : topic diff=0.167087, rho=0.281166\n",
      "2020-11-08 20:14:02,071 : INFO : PROGRESS: pass 4, at document #14000/15299\n",
      "2020-11-08 20:14:02,495 : INFO : merging changes from 2000 documents into a model of 15299 documents\n",
      "2020-11-08 20:14:02,503 : INFO : topic #0 (0.333): 0.009*\"poster\" + 0.006*\"learning\" + 0.006*\"workshop\" + 0.005*\"work\" + 0.005*\"neurips\" + 0.004*\"today\" + 0.004*\"ai\" + 0.004*\"come\" + 0.004*\"paper\" + 0.003*\"research\"\n",
      "2020-11-08 20:14:02,505 : INFO : topic #1 (0.333): 0.013*\"learning\" + 0.007*\"talk\" + 0.007*\"workshop\" + 0.006*\"ai\" + 0.005*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.004*\"bengio\" + 0.004*\"deep learning\" + 0.003*\"machine\"\n",
      "2020-11-08 20:14:02,507 : INFO : topic #2 (0.333): 0.007*\"ai\" + 0.004*\"poster\" + 0.003*\"great\" + 0.003*\"work\" + 0.003*\"workshop\" + 0.003*\"vancouver\" + 0.003*\"paper\" + 0.003*\"talk\" + 0.003*\"tweet\" + 0.003*\"ml\"\n",
      "2020-11-08 20:14:02,508 : INFO : topic diff=0.174714, rho=0.281166\n",
      "2020-11-08 20:14:03,050 : INFO : -9.852 per-word bound, 924.0 perplexity estimate based on a held-out corpus of 1299 documents with 27022 words\n",
      "2020-11-08 20:14:03,050 : INFO : PROGRESS: pass 4, at document #15299/15299\n",
      "2020-11-08 20:14:03,316 : INFO : merging changes from 1299 documents into a model of 15299 documents\n",
      "2020-11-08 20:14:03,325 : INFO : topic #0 (0.333): 0.009*\"poster\" + 0.006*\"learning\" + 0.005*\"work\" + 0.005*\"workshop\" + 0.005*\"today\" + 0.005*\"neurips\" + 0.004*\"come\" + 0.004*\"ai\" + 0.004*\"hall\" + 0.003*\"paper\"\n",
      "2020-11-08 20:14:03,327 : INFO : topic #1 (0.333): 0.012*\"learning\" + 0.007*\"talk\" + 0.005*\"workshop\" + 0.005*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"deep learning\" + 0.003*\"machine\" + 0.003*\"bengio\"\n",
      "2020-11-08 20:14:03,329 : INFO : topic #2 (0.333): 0.006*\"ai\" + 0.004*\"poster\" + 0.004*\"vancouver\" + 0.004*\"great\" + 0.003*\"work\" + 0.003*\"tweet\" + 0.003*\"workshop\" + 0.003*\"talk\" + 0.003*\"paper\" + 0.002*\"ml\"\n",
      "2020-11-08 20:14:03,330 : INFO : topic diff=0.180309, rho=0.281166\n"
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=gs_corpus, num_topics=3, id2word=id2word, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-08 20:16:28,128 : INFO : topic #0 (0.333): 0.009*\"poster\" + 0.006*\"learning\" + 0.005*\"work\" + 0.005*\"workshop\" + 0.005*\"today\" + 0.005*\"neurips\" + 0.004*\"come\" + 0.004*\"ai\" + 0.004*\"hall\" + 0.003*\"paper\"\n",
      "2020-11-08 20:16:28,131 : INFO : topic #1 (0.333): 0.012*\"learning\" + 0.007*\"talk\" + 0.005*\"workshop\" + 0.005*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"deep learning\" + 0.003*\"machine\" + 0.003*\"bengio\"\n",
      "2020-11-08 20:16:28,133 : INFO : topic #2 (0.333): 0.006*\"ai\" + 0.004*\"poster\" + 0.004*\"vancouver\" + 0.004*\"great\" + 0.003*\"work\" + 0.003*\"tweet\" + 0.003*\"workshop\" + 0.003*\"talk\" + 0.003*\"paper\" + 0.002*\"ml\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"poster\" + 0.006*\"learning\" + 0.005*\"work\" + 0.005*\"workshop\" + 0.005*\"today\" + 0.005*\"neurips\" + 0.004*\"come\" + 0.004*\"ai\" + 0.004*\"hall\" + 0.003*\"paper\"'),\n",
       " (1,\n",
       "  '0.012*\"learning\" + 0.007*\"talk\" + 0.005*\"workshop\" + 0.005*\"ai\" + 0.004*\"ml\" + 0.004*\"deep\" + 0.004*\"neurips\" + 0.003*\"deep learning\" + 0.003*\"machine\" + 0.003*\"bengio\"'),\n",
       " (2,\n",
       "  '0.006*\"ai\" + 0.004*\"poster\" + 0.004*\"vancouver\" + 0.004*\"great\" + 0.003*\"work\" + 0.003*\"tweet\" + 0.003*\"workshop\" + 0.003*\"talk\" + 0.003*\"paper\" + 0.002*\"ml\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x7f9591e41580>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_corpus = lda[gs_corpus]\n",
    "lda_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.01832504), (1, 0.01683113), (2, 0.9648438)],\n",
       " [(0, 0.01989959), (1, 0.95987207), (2, 0.02022829)],\n",
       " [(0, 0.97782177), (1, 0.011347768), (2, 0.010830506)],\n",
       " [(0, 0.9672344), (1, 0.010698692), (2, 0.022066943)],\n",
       " [(0, 0.011756039), (1, 0.011537025), (2, 0.9767069)]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_docs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order of operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MDS\n",
    "1. TSNE\n",
    "1. Cosine similarity vs.\n",
    "1. Euclidian similarity\n",
    "1. NLTK > SpaCy (small OK, can try medium)\n",
    "1. Define tokenize and stem functions, plus list of stop words.\n",
    "1. Apply Tf-Idf transformation, and compute matrix of cosine-similarity distances.\n",
    "1. Perform multidimensional scale (MDS) transformation.\n",
    "1. Plot the result of MDS.\n",
    "1. Compare with TSNE\n",
    "1. ScatterText\n",
    "1. Word2Vec\n",
    "1. Apply K-Means clustering with pre-specified number of clusters, and visualize result with MDS.\n",
    "1. Apply LDA transformation with pre-specified number of topics, and with or without topic seeding.\n",
    "1. PyLDAVis\n",
    "1. Visualize result of LDA transformation with MDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
